#!/usr/bin/env python3
"""Autogenerated MCP bundle for crypto_key_gen (includes metrics wrapper)."""
from __future__ import annotations

import json
import os
import time
from typing import Any, Callable, Dict, Optional, Tuple

# --- Metrics wrapper (from main.py) ---
FunctionCallable = Callable[..., Any]

_PROCESS_START_UNIX = time.time()

_FIRST_INVOKE: dict[str, bool] = {}

def _safe_get_max_rss_kb() -> Optional[int]:
    """Best-effort max RSS in KB (Linux: ru_maxrss is KB)."""
    try:
        import resource  # stdlib on Linux

        return int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)
    except Exception:
        return None

def _estimate_request_bytes(request: Any) -> Optional[int]:
    """Estimate incoming request bytes without breaking downstream handlers."""
    try:
        cl = getattr(request, "content_length", None)
        if isinstance(cl, int) and cl >= 0:
            return cl
    except Exception:
        pass

    # Fall back to reading request body with caching enabled, so handlers can still read it.
    try:
        if hasattr(request, "get_data"):
            data = request.get_data(cache=True)  # important: cache=True
            if data is not None:
                return int(len(data))
    except Exception:
        pass

    return None

def _normalize_handler_return(rv: Any) -> Tuple[bytes, int, Dict[str, str]]:
    """
    Normalize common Functions Framework return shapes:
      - (body, status, headers)
      - (body, status)
      - body
      - flask.Response-like object
    """
    body: Any = rv
    status: int = 200
    headers: Dict[str, str] = {}

    # Tuple/list return
    if isinstance(rv, (tuple, list)):
        if len(rv) >= 1:
            body = rv[0]
        if len(rv) >= 2 and rv[1] is not None:
            try:
                status = int(rv[1])
            except Exception:
                status = 200
        if len(rv) >= 3 and isinstance(rv[2], dict):
            headers = {str(k): str(v) for k, v in rv[2].items()}

    # flask.Response-like
    if hasattr(body, "get_data") and callable(getattr(body, "get_data")):
        try:
            data = body.get_data()  # bytes
            st = getattr(body, "status_code", None)
            if isinstance(st, int):
                status = st
            hdrs = getattr(body, "headers", None)
            if hdrs is not None:
                try:
                    headers = {str(k): str(v) for k, v in dict(hdrs).items()}
                except Exception:
                    pass
            return bytes(data), status, headers
        except Exception:
            pass

    # Body encoding
    if body is None:
        body_bytes = b""
    elif isinstance(body, (bytes, bytearray)):
        body_bytes = bytes(body)
    elif isinstance(body, str):
        body_bytes = body.encode("utf-8", errors="replace")
    elif isinstance(body, (dict, list)):
        body_bytes = json.dumps(body, ensure_ascii=False).encode("utf-8")
    else:
        body_bytes = str(body).encode("utf-8", errors="replace")

    return body_bytes, status, headers

def _sanitize_response_json_for_logs(parsed: Any) -> Optional[dict]:
    """
    If response is JSON, keep small metadata fields but avoid logging huge blobs (base64, etc.).
    """
    if not isinstance(parsed, dict):
        return None

    drop_keys = {
        "converted_image",
        "processed_data",
        "converted_image_base64",
        "processed_data_base64",
    }  # large base64 fields in your samples
    out: dict[str, Any] = {}

    for k, v in parsed.items():
        if k in drop_keys:
            continue
        # Avoid very large strings in logs
        if isinstance(v, str) and len(v) > 512:
            out[k] = v[:512] + "...(truncated)"
        else:
            out[k] = v
    return out

def _emit_metrics_log(line: dict) -> None:
    """
    Emit one structured JSON log line to stdout.
    Cloud Run/Cloud Logging will ingest it as a structured payload.
    """
    try:
        print(json.dumps(line, ensure_ascii=False))
    except Exception:
        # Best-effort fallback
        print(str(line))

def _with_metrics(function_id: str, handler: FunctionCallable) -> FunctionCallable:
    """Wrap a Functions Framework handler with timing/size/memory instrumentation."""

    def _wrapped(*args: Any, **kwargs: Any) -> Any:
        request = args[0] if args else kwargs.get("request")

        cold_start = not _FIRST_INVOKE.get(function_id, False)
        _FIRST_INVOKE[function_id] = True

        req_bytes = _estimate_request_bytes(request)

        wall_t0 = time.perf_counter()
        cpu_t0 = time.process_time()

        error: Optional[str] = None
        error_type: Optional[str] = None
        rv: Any = None

        try:
            rv = handler(*args, **kwargs)
        except Exception as exc:
            error_type = type(exc).__name__
            error = str(exc)
            raise
        finally:
            wall_t1 = time.perf_counter()
            cpu_t1 = time.process_time()

            body_bytes: bytes = b""
            status_code: Optional[int] = None
            resp_bytes: Optional[int] = None
            resp_meta: Optional[dict] = None

            try:
                body_bytes, status_code, _headers = _normalize_handler_return(rv)
                resp_bytes = len(body_bytes)

                # If body looks like JSON, parse and keep a sanitized subset for logs
                try:
                    parsed = json.loads(body_bytes.decode("utf-8", errors="replace"))
                    resp_meta = _sanitize_response_json_for_logs(parsed)
                except Exception:
                    resp_meta = None
            except Exception:
                # Never let metrics break the request path.
                pass

            log_line = {
                "type": "invocation_metrics",
                "function_id": function_id,
                "cold_start": cold_start,
                "process_uptime_s": round(time.time() - _PROCESS_START_UNIX, 3),
                "wall_ms": round((wall_t1 - wall_t0) * 1000.0, 3),
                "cpu_ms": round((cpu_t1 - cpu_t0) * 1000.0, 3),
                "max_rss_kb": _safe_get_max_rss_kb(),
                "request_bytes": req_bytes,
                "response_bytes": resp_bytes,
                "status_code": status_code,
                "error_type": error_type,
                "error": error,
                # Helpful Cloud Run env metadata (if present)
                "k_service": os.getenv("K_SERVICE"),
                "k_revision": os.getenv("K_REVISION"),
                "k_configuration": os.getenv("K_CONFIGURATION"),
                # Optional: request metadata (best-effort)
                "http_method": getattr(request, "method", None),
                "http_path": getattr(request, "path", None),
            }
            if resp_meta is not None:
                log_line["response_meta"] = resp_meta

            _emit_metrics_log(log_line)

        return rv

    _wrapped.__name__ = getattr(handler, "__name__", function_id)
    _wrapped.__doc__ = getattr(handler, "__doc__", None)
    return _wrapped

# --- Handler module code ---
"""Generate RSA key pairs to exercise sustained CPU work (tunable).

Evaluation-oriented behavior:
- Keeps response small (no private key material).
- Allows tuning runtime via `iterations` and/or `target_ms`.

Request JSON (all optional):
- bits / key_bits: one of {2048, 3072, 4096, 8192} (default: 3072)
- public_exponent: RSA public exponent (default: 65537)
- iterations: number of RSA keypairs to generate (default: 1)
- target_ms: minimum wall time to spend (default: 0 => no minimum)
- burn_chunk_kb: chunk size for CPU-burn hashing (default: 256)
- return_public_key_pem: whether to return full public key PEM (default: false)

Notes:
- If `target_ms` exceeds the time needed for RSA generation, we burn CPU time by hashing
  a rolling buffer until the minimum wall time is reached. This produces more stable
  runtimes across regions/CPU allocations than relying purely on RSA generation loops.
"""


import hashlib
import json
import os
import time
from typing import Dict, Tuple

import functions_framework
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import rsa

_ALLOWED_BITS = (2048, 3072, 4096, 8192)


def _parse_bits(payload: dict) -> int:
    bits = payload.get("bits", payload.get("key_bits", 3072))
    try:
        bits = int(bits)
    except Exception:
        bits = 3072
    if bits not in _ALLOWED_BITS:
        bits = 3072
    return bits


def _parse_int(payload: dict, key: str, default: int, minimum: int | None = None, maximum: int | None = None) -> int:
    try:
        val = int(payload.get(key, default))
    except Exception:
        val = default
    if minimum is not None:
        val = max(minimum, val)
    if maximum is not None:
        val = min(maximum, val)
    return val


def _burn_cpu_until(deadline_s: float, chunk_kb: int) -> Tuple[int, str]:
    """Burn CPU by hashing a rolling buffer; returns (hash_iterations, final_digest)."""
    data = bytearray(os.urandom(max(1, chunk_kb) * 1024))
    digest = b""
    iters = 0
    while time.perf_counter() < deadline_s:
        digest = hashlib.sha256(data).digest()
        # mutate buffer to prevent trivial caching
        for i in range(min(32, len(data))):
            data[i] ^= digest[i % len(digest)]
        iters += 1
    return iters, digest.hex() if digest else ""


@functions_framework.http
def crypto_key_gen(request) -> tuple[str, int, Dict[str, str]]:
    """Generate RSA public keys and optionally burn CPU until a target duration is met."""
    payload = request.get_json(silent=True) or {}

    bits = _parse_bits(payload)
    public_exponent = _parse_int(payload, "public_exponent", 65537, minimum=3, maximum=2**31 - 1)

    iterations = _parse_int(payload, "iterations", 1, minimum=1, maximum=50)
    target_ms = _parse_int(payload, "target_ms", 0, minimum=0, maximum=15 * 60 * 1000)  # cap 15 min
    burn_chunk_kb = _parse_int(payload, "burn_chunk_kb", 256, minimum=1, maximum=4096)
    return_public_key_pem = bool(payload.get("return_public_key_pem", False))

    wall_start = time.perf_counter()
    cpu_start = time.process_time()

    public_keys: list[bytes] = []

    # Step 1: deterministic crypto work
    for _ in range(iterations):
        private_key = rsa.generate_private_key(public_exponent=public_exponent, key_size=bits)
        pub = private_key.public_key()
        public_bytes = pub.public_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PublicFormat.SubjectPublicKeyInfo,
        )
        public_keys.append(public_bytes)

    # Step 2: optional CPU burn to reach target_ms
    burn_iters = 0
    burn_digest = ""
    if target_ms > 0:
        deadline_s = wall_start + (target_ms / 1000.0)
        if time.perf_counter() < deadline_s:
            burn_iters, burn_digest = _burn_cpu_until(deadline_s, burn_chunk_kb)

    wall_ms = round((time.perf_counter() - wall_start) * 1000.0, 3)
    cpu_ms = round((time.process_time() - cpu_start) * 1000.0, 3)

    response: dict = {
        "scenario": "Long runtime + Little data",
        "key_bits": bits,
        "public_exponent": public_exponent,
        "iterations": iterations,
        "target_ms": target_ms,
        "burn_hash_iterations": burn_iters,
        "burn_final_digest": burn_digest or None,
        # Optional internal timings (your main wrapper logs the authoritative ones)
        "wall_ms_internal": wall_ms,
        "cpu_ms_internal": cpu_ms,
    }

    if return_public_key_pem:
        # Keep response bounded: if multiple keys were generated, return only the last one.
        response["public_key_pem"] = public_keys[-1].decode("utf-8", errors="ignore") if public_keys else None

    return json.dumps(response), 200, {"Content-Type": "application/json"}

# --- Wrapped entrypoints ---
_raw_handler = crypto_key_gen
crypto_key_gen = _with_metrics("crypto_key_gen", _raw_handler)

def main(request):
    return crypto_key_gen(request)
