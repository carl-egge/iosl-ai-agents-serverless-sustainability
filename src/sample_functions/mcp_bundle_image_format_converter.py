#!/usr/bin/env python3
"""Autogenerated MCP bundle for image_format_converter (includes metrics wrapper)."""
from __future__ import annotations

import json
import os
import time
from typing import Any, Callable, Dict, Optional, Tuple

# --- Metrics wrapper (from main.py) ---
FunctionCallable = Callable[..., Any]

_PROCESS_START_UNIX = time.time()

_FIRST_INVOKE: dict[str, bool] = {}

def _safe_get_max_rss_kb() -> Optional[int]:
    """Best-effort max RSS in KB (Linux: ru_maxrss is KB)."""
    try:
        import resource  # stdlib on Linux

        return int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)
    except Exception:
        return None

def _estimate_request_bytes(request: Any) -> Optional[int]:
    """Estimate incoming request bytes without breaking downstream handlers."""
    try:
        cl = getattr(request, "content_length", None)
        if isinstance(cl, int) and cl >= 0:
            return cl
    except Exception:
        pass

    # Fall back to reading request body with caching enabled, so handlers can still read it.
    try:
        if hasattr(request, "get_data"):
            data = request.get_data(cache=True)  # important: cache=True
            if data is not None:
                return int(len(data))
    except Exception:
        pass

    return None

def _normalize_handler_return(rv: Any) -> Tuple[bytes, int, Dict[str, str]]:
    """
    Normalize common Functions Framework return shapes:
      - (body, status, headers)
      - (body, status)
      - body
      - flask.Response-like object
    """
    body: Any = rv
    status: int = 200
    headers: Dict[str, str] = {}

    # Tuple/list return
    if isinstance(rv, (tuple, list)):
        if len(rv) >= 1:
            body = rv[0]
        if len(rv) >= 2 and rv[1] is not None:
            try:
                status = int(rv[1])
            except Exception:
                status = 200
        if len(rv) >= 3 and isinstance(rv[2], dict):
            headers = {str(k): str(v) for k, v in rv[2].items()}

    # flask.Response-like
    if hasattr(body, "get_data") and callable(getattr(body, "get_data")):
        try:
            data = body.get_data()  # bytes
            st = getattr(body, "status_code", None)
            if isinstance(st, int):
                status = st
            hdrs = getattr(body, "headers", None)
            if hdrs is not None:
                try:
                    headers = {str(k): str(v) for k, v in dict(hdrs).items()}
                except Exception:
                    pass
            return bytes(data), status, headers
        except Exception:
            pass

    # Body encoding
    if body is None:
        body_bytes = b""
    elif isinstance(body, (bytes, bytearray)):
        body_bytes = bytes(body)
    elif isinstance(body, str):
        body_bytes = body.encode("utf-8", errors="replace")
    elif isinstance(body, (dict, list)):
        body_bytes = json.dumps(body, ensure_ascii=False).encode("utf-8")
    else:
        body_bytes = str(body).encode("utf-8", errors="replace")

    return body_bytes, status, headers

def _sanitize_response_json_for_logs(parsed: Any) -> Optional[dict]:
    """
    If response is JSON, keep small metadata fields but avoid logging huge blobs (base64, etc.).
    """
    if not isinstance(parsed, dict):
        return None

    drop_keys = {
        "converted_image",
        "processed_data",
        "converted_image_base64",
        "processed_data_base64",
    }  # large base64 fields in your samples
    out: dict[str, Any] = {}

    for k, v in parsed.items():
        if k in drop_keys:
            continue
        # Avoid very large strings in logs
        if isinstance(v, str) and len(v) > 512:
            out[k] = v[:512] + "...(truncated)"
        else:
            out[k] = v
    return out

def _emit_metrics_log(line: dict) -> None:
    """
    Emit one structured JSON log line to stdout.
    Cloud Run/Cloud Logging will ingest it as a structured payload.
    """
    try:
        print(json.dumps(line, ensure_ascii=False))
    except Exception:
        # Best-effort fallback
        print(str(line))

def _with_metrics(function_id: str, handler: FunctionCallable) -> FunctionCallable:
    """Wrap a Functions Framework handler with timing/size/memory instrumentation."""

    def _wrapped(*args: Any, **kwargs: Any) -> Any:
        request = args[0] if args else kwargs.get("request")

        cold_start = not _FIRST_INVOKE.get(function_id, False)
        _FIRST_INVOKE[function_id] = True

        req_bytes = _estimate_request_bytes(request)

        wall_t0 = time.perf_counter()
        cpu_t0 = time.process_time()

        error: Optional[str] = None
        error_type: Optional[str] = None
        rv: Any = None

        try:
            rv = handler(*args, **kwargs)
        except Exception as exc:
            error_type = type(exc).__name__
            error = str(exc)
            raise
        finally:
            wall_t1 = time.perf_counter()
            cpu_t1 = time.process_time()

            body_bytes: bytes = b""
            status_code: Optional[int] = None
            resp_bytes: Optional[int] = None
            resp_meta: Optional[dict] = None

            try:
                body_bytes, status_code, _headers = _normalize_handler_return(rv)
                resp_bytes = len(body_bytes)

                # If body looks like JSON, parse and keep a sanitized subset for logs
                try:
                    parsed = json.loads(body_bytes.decode("utf-8", errors="replace"))
                    resp_meta = _sanitize_response_json_for_logs(parsed)
                except Exception:
                    resp_meta = None
            except Exception:
                # Never let metrics break the request path.
                pass

            log_line = {
                "type": "invocation_metrics",
                "function_id": function_id,
                "cold_start": cold_start,
                "process_uptime_s": round(time.time() - _PROCESS_START_UNIX, 3),
                "wall_ms": round((wall_t1 - wall_t0) * 1000.0, 3),
                "cpu_ms": round((cpu_t1 - cpu_t0) * 1000.0, 3),
                "max_rss_kb": _safe_get_max_rss_kb(),
                "request_bytes": req_bytes,
                "response_bytes": resp_bytes,
                "status_code": status_code,
                "error_type": error_type,
                "error": error,
                # Helpful Cloud Run env metadata (if present)
                "k_service": os.getenv("K_SERVICE"),
                "k_revision": os.getenv("K_REVISION"),
                "k_configuration": os.getenv("K_CONFIGURATION"),
                # Optional: request metadata (best-effort)
                "http_method": getattr(request, "method", None),
                "http_path": getattr(request, "path", None),
            }
            if resp_meta is not None:
                log_line["response_meta"] = resp_meta

            _emit_metrics_log(log_line)

        return rv

    _wrapped.__name__ = getattr(handler, "__name__", function_id)
    _wrapped.__doc__ = getattr(handler, "__doc__", None)
    return _wrapped

# --- Handler module code ---
"""Image converter for evaluation runs.

Evaluation improvements:
- Default behavior writes converted output to GCS and returns only metadata + output URI.
- Inline (base64) output is still available via `return_inline: true` for debugging.

Inputs supported (JSON only):
- base64 field `data`
- pointer: `gcs_uri=gs://bucket/path` or (`bucket`, `object`)

Request JSON (selected fields):
- format / output_format: e.g. WEBP, PNG, JPEG (default: WEBP)
- quality: 30..100 (default: 80)
- return_inline: bool (default: false)
- output_gcs_uri: gs://bucket/path (optional)
- output_bucket / output_object: alternative output location (optional)
- output_prefix: if output not specified and input came from GCS or DEFAULT_OUTPUT_BUCKET is set,
  write to {bucket}/{output_prefix}/<uuid>.<ext> (default prefix: eval_outputs/image_format_converter)

Environment variables (optional):
- DEFAULT_OUTPUT_BUCKET: bucket used if output location is not provided and input is not from GCS
- DEFAULT_OUTPUT_PREFIX_IMAGE: default output prefix (default: eval_outputs/image_format_converter)
- MAX_INLINE_MB: inline response limit in MB (default: 16)
"""


import base64
import binascii
import io
import json
import os
import uuid
from typing import Dict, Optional, Tuple

import functions_framework
from google.cloud import storage
from PIL import Image

_STORAGE_CLIENT: Optional[storage.Client] = None
_MAX_INLINE_BYTES: int = 16 * 1024 * 1024

try:
    _MAX_INLINE_BYTES = max(1, int(os.getenv("MAX_INLINE_MB", "16"))) * 1024 * 1024
except Exception:
    _MAX_INLINE_BYTES = 16 * 1024 * 1024

_DEFAULT_OUTPUT_BUCKET = os.getenv("DEFAULT_OUTPUT_BUCKET", "").strip() or None
_DEFAULT_OUTPUT_PREFIX = os.getenv(
    "DEFAULT_OUTPUT_PREFIX_IMAGE", "eval_outputs/image_format_converter"
).strip()


def _parse_gcs_uri(uri: Optional[str]) -> Optional[Tuple[str, str]]:
    if not uri:
        return None
    if uri.startswith("gs://"):
        uri = uri[5:]
    parts = uri.split("/", 1)
    if len(parts) == 2 and parts[0] and parts[1]:
        return parts[0], parts[1]
    return None


def _get_storage_client() -> storage.Client:
    global _STORAGE_CLIENT
    if _STORAGE_CLIENT is None:
        _STORAGE_CLIENT = storage.Client()
    return _STORAGE_CLIENT


def _download_from_bucket(bucket_name: str, object_path: str) -> bytes:
    bucket = _get_storage_client().bucket(bucket_name)
    blob = bucket.blob(object_path)
    return blob.download_as_bytes()


def _upload_to_bucket(bucket_name: str, object_path: str, data: bytes, content_type: str) -> str:
    bucket = _get_storage_client().bucket(bucket_name)
    blob = bucket.blob(object_path)
    blob.upload_from_string(data, content_type=content_type)
    return f"gs://{bucket_name}/{object_path}"


def _load_image_bytes(payload: dict) -> Tuple[bytes, Optional[str]]:
    """Return (bytes, input_gcs_uri_if_any)."""
    encoded = payload.get("data")
    if isinstance(encoded, str):
        try:
            return base64.b64decode(encoded, validate=True), None
        except (ValueError, binascii.Error):
            raise ValueError("Invalid base64 in `data`.")

    gcs_location = _parse_gcs_uri(payload.get("gcs_uri"))
    bucket = payload.get("bucket")
    object_path = payload.get("object")
    if gcs_location:
        bucket, object_path = gcs_location
    if bucket and object_path:
        return _download_from_bucket(str(bucket), str(object_path)), f"gs://{bucket}/{object_path}"

    raise ValueError(
        "Send JSON with base64 `data`, or specify `bucket`/`object` or `gcs_uri=gs://bucket/path`."
    )


def _pick_output_location(payload: dict, input_gcs_uri: Optional[str], ext: str) -> Tuple[str, str]:
    # Explicit output_gcs_uri
    out_gcs = payload.get("output_gcs_uri") or payload.get("output_gcs")
    parsed = _parse_gcs_uri(out_gcs) if isinstance(out_gcs, str) else None
    if parsed:
        return parsed

    # Explicit output_bucket/object
    out_bucket = payload.get("output_bucket")
    out_object = payload.get("output_object")
    if out_bucket and out_object:
        return str(out_bucket), str(out_object)

    # Derive from input bucket if possible
    derived_bucket: Optional[str] = None
    if input_gcs_uri:
        parsed_in = _parse_gcs_uri(input_gcs_uri)
        if parsed_in:
            derived_bucket = parsed_in[0]
    if not derived_bucket:
        derived_bucket = _DEFAULT_OUTPUT_BUCKET

    if not derived_bucket:
        raise ValueError(
            "No output location specified. Provide `output_gcs_uri` or (`output_bucket`,`output_object`), "
            "or set DEFAULT_OUTPUT_BUCKET env var, or provide input via GCS so output can be derived."
        )

    prefix = str(payload.get("output_prefix") or _DEFAULT_OUTPUT_PREFIX).strip().strip("/")
    object_path = f"{prefix}/{uuid.uuid4().hex}.{ext}"
    return derived_bucket, object_path


@functions_framework.http
def image_format_converter(request) -> tuple[str, int, Dict[str, str]]:
    """Convert uploaded image bytes into the requested format."""
    payload = request.get_json(silent=True)
    if not isinstance(payload, dict):
        return json.dumps({"error": "Expected a JSON object payload."}), 400, {"Content-Type": "application/json"}

    # Accept both keys for compatibility.
    target_format = str(payload.get("format", payload.get("output_format", "WEBP"))).upper()
    quality = int(payload.get("quality", 80))
    quality = max(30, min(quality, 100))

    return_inline = bool(payload.get("return_inline", False))

    try:
        input_bytes, input_gcs_uri = _load_image_bytes(payload)
    except ValueError as error:
        return json.dumps({"error": str(error)}), 400, {"Content-Type": "application/json"}

    try:
        image = Image.open(io.BytesIO(input_bytes))
    except Exception as error:
        return json.dumps({"error": f"Failed to decode image: {error}"}), 400, {"Content-Type": "application/json"}

    output_buffer = io.BytesIO()
    try:
        image.save(output_buffer, format=target_format, quality=quality)
    except Exception as error:
        return (
            json.dumps({"error": f"Failed to encode image as {target_format}: {error}"}),
            400,
            {"Content-Type": "application/json"},
        )

    output_bytes = output_buffer.getvalue()

    response: dict = {
        "scenario": "Short runtime + Large data",
        "format": target_format,
        "quality": quality,
        "input_bytes": len(input_bytes),
        "output_bytes": len(output_bytes),
        "input_size_mb": round(len(input_bytes) / (1024 * 1024), 4),
        "output_size_mb": round(len(output_bytes) / (1024 * 1024), 4),
        "input_gcs_uri": input_gcs_uri,
        "return_inline": return_inline,
    }

    if return_inline:
        if len(output_bytes) > _MAX_INLINE_BYTES:
            return (
                json.dumps(
                    {
                        "error": "Inline response too large.",
                        "hint": "Use GCS output or lower the input size.",
                        "max_inline_bytes": _MAX_INLINE_BYTES,
                    }
                ),
                413,
                {"Content-Type": "application/json"},
            )
        # Debug mode only (can be large).
        response["converted_image_base64"] = base64.b64encode(output_bytes).decode("utf-8")
        return json.dumps(response), 200, {"Content-Type": "application/json"}

    # Evaluation mode: write output to GCS and return URI + metadata.
    ext = target_format.lower()
    if ext == "jpeg":
        ext = "jpg"

    try:
        out_bucket, out_object = _pick_output_location(payload, input_gcs_uri, ext)
        content_type = f"image/{'jpeg' if ext == 'jpg' else ext}"
        output_gcs_uri = _upload_to_bucket(out_bucket, out_object, output_bytes, content_type=content_type)
        response["output_gcs_uri"] = output_gcs_uri
        response["output_bucket"] = out_bucket
        response["output_object"] = out_object
    except ValueError as error:
        return (
            json.dumps(
                {
                    "error": str(error),
                    "hint": "Set output_gcs_uri or output_bucket/output_object or DEFAULT_OUTPUT_BUCKET.",
                }
            ),
            400,
            {"Content-Type": "application/json"},
        )
    except Exception as error:
        return json.dumps({"error": f"Failed to upload output to GCS: {error}"}), 500, {"Content-Type": "application/json"}

    return json.dumps(response), 200, {"Content-Type": "application/json"}

# --- Wrapped entrypoints ---
_raw_handler = image_format_converter
image_format_converter = _with_metrics("image_format_converter", _raw_handler)

def main(request):
    return image_format_converter(request)
