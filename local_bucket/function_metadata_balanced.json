{
  "description": "Function metadata for carbon-aware scheduling",
  "functions": {
    "api_health_check": {
      "function_id": "api_health_check",
      "runtime_ms": 0.073,
      "memory_mb": 256,
      "description": "Minimal REST health snapshot designed for extremely high throughput and very little payload.",
      "data_input_gb": 0.0,
      "data_output_gb": 1.23e-06,
      "source_location": "us-east1",
      "invocations_per_day": 20000,
      "priority": "balanced",
      "latency_important": true,
      "allowed_regions": [
        "europe-north2",
        "us-east1",
        "northamerica-northeast1",
        "us-central1",
        "europe-west1"
      ],
      "code": "#!/usr/bin/env python3\n\"\"\"Autogenerated MCP bundle for api_health_check (includes metrics wrapper).\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\n# --- Metrics wrapper (from main.py) ---\nFunctionCallable = Callable[..., Any]\n\n_PROCESS_START_UNIX = time.time()\n\n_FIRST_INVOKE: dict[str, bool] = {}\n\ndef _safe_get_max_rss_kb() -> Optional[int]:\n    \"\"\"Best-effort max RSS in KB (Linux: ru_maxrss is KB).\"\"\"\n    try:\n        import resource  # stdlib on Linux\n\n        return int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n    except Exception:\n        return None\n\ndef _estimate_request_bytes(request: Any) -> Optional[int]:\n    \"\"\"Estimate incoming request bytes without breaking downstream handlers.\"\"\"\n    try:\n        cl = getattr(request, \"content_length\", None)\n        if isinstance(cl, int) and cl >= 0:\n            return cl\n    except Exception:\n        pass\n\n    # Fall back to reading request body with caching enabled, so handlers can still read it.\n    try:\n        if hasattr(request, \"get_data\"):\n            data = request.get_data(cache=True)  # important: cache=True\n            if data is not None:\n                return int(len(data))\n    except Exception:\n        pass\n\n    return None\n\ndef _normalize_handler_return(rv: Any) -> Tuple[bytes, int, Dict[str, str]]:\n    \"\"\"\n    Normalize common Functions Framework return shapes:\n      - (body, status, headers)\n      - (body, status)\n      - body\n      - flask.Response-like object\n    \"\"\"\n    body: Any = rv\n    status: int = 200\n    headers: Dict[str, str] = {}\n\n    # Tuple/list return\n    if isinstance(rv, (tuple, list)):\n        if len(rv) >= 1:\n            body = rv[0]\n        if len(rv) >= 2 and rv[1] is not None:\n            try:\n                status = int(rv[1])\n            except Exception:\n                status = 200\n        if len(rv) >= 3 and isinstance(rv[2], dict):\n            headers = {str(k): str(v) for k, v in rv[2].items()}\n\n    # flask.Response-like\n    if hasattr(body, \"get_data\") and callable(getattr(body, \"get_data\")):\n        try:\n            data = body.get_data()  # bytes\n            st = getattr(body, \"status_code\", None)\n            if isinstance(st, int):\n                status = st\n            hdrs = getattr(body, \"headers\", None)\n            if hdrs is not None:\n                try:\n                    headers = {str(k): str(v) for k, v in dict(hdrs).items()}\n                except Exception:\n                    pass\n            return bytes(data), status, headers\n        except Exception:\n            pass\n\n    # Body encoding\n    if body is None:\n        body_bytes = b\"\"\n    elif isinstance(body, (bytes, bytearray)):\n        body_bytes = bytes(body)\n    elif isinstance(body, str):\n        body_bytes = body.encode(\"utf-8\", errors=\"replace\")\n    elif isinstance(body, (dict, list)):\n        body_bytes = json.dumps(body, ensure_ascii=False).encode(\"utf-8\")\n    else:\n        body_bytes = str(body).encode(\"utf-8\", errors=\"replace\")\n\n    return body_bytes, status, headers\n\ndef _sanitize_response_json_for_logs(parsed: Any) -> Optional[dict]:\n    \"\"\"\n    If response is JSON, keep small metadata fields but avoid logging huge blobs (base64, etc.).\n    \"\"\"\n    if not isinstance(parsed, dict):\n        return None\n\n    drop_keys = {\n        \"converted_image\",\n        \"processed_data\",\n        \"converted_image_base64\",\n        \"processed_data_base64\",\n    }  # large base64 fields in your samples\n    out: dict[str, Any] = {}\n\n    for k, v in parsed.items():\n        if k in drop_keys:\n            continue\n        # Avoid very large strings in logs\n        if isinstance(v, str) and len(v) > 512:\n            out[k] = v[:512] + \"...(truncated)\"\n        else:\n            out[k] = v\n    return out\n\ndef _emit_metrics_log(line: dict) -> None:\n    \"\"\"\n    Emit one structured JSON log line to stdout.\n    Cloud Run/Cloud Logging will ingest it as a structured payload.\n    \"\"\"\n    try:\n        print(json.dumps(line, ensure_ascii=False))\n    except Exception:\n        # Best-effort fallback\n        print(str(line))\n\ndef _parse_utc_timestamp(value: Any) -> Optional[datetime]:\n    if value is None:\n        return None\n    if isinstance(value, datetime):\n        dt = value\n    elif isinstance(value, str):\n        text = value.replace(\"Z\", \"+00:00\")\n        try:\n            dt = datetime.fromisoformat(text)\n        except ValueError:\n            return None\n    else:\n        return None\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=timezone.utc)\n    return dt.astimezone(timezone.utc)\n\ndef _extract_request_metadata(request: Any) -> Dict[str, Optional[str]]:\n    event_id = None\n    dispatch_sent_time_utc = None\n\n    if request is None:\n        return {\"event_id\": None, \"dispatch_sent_time_utc\": None}\n\n    payload = None\n    try:\n        if hasattr(request, \"get_json\"):\n            payload = request.get_json(silent=True)\n    except Exception:\n        payload = None\n\n    if isinstance(payload, dict):\n        event_id = payload.get(\"event_id\")\n        dispatch_sent_time_utc = payload.get(\"dispatch_sent_time_utc\")\n\n    try:\n        args = getattr(request, \"args\", None)\n        if args is not None:\n            if event_id is None:\n                event_id = args.get(\"event_id\")\n            if dispatch_sent_time_utc is None:\n                dispatch_sent_time_utc = args.get(\"dispatch_sent_time_utc\")\n    except Exception:\n        pass\n\n    return {\n        \"event_id\": event_id,\n        \"dispatch_sent_time_utc\": dispatch_sent_time_utc,\n    }\n\ndef _with_metrics(function_id: str, handler: FunctionCallable) -> FunctionCallable:\n    \"\"\"Wrap a Functions Framework handler with timing/size/memory instrumentation.\"\"\"\n\n    def _wrapped(*args: Any, **kwargs: Any) -> Any:\n        request = args[0] if args else kwargs.get(\"request\")\n        request_received_dt = datetime.now(timezone.utc)\n        request_meta = _extract_request_metadata(request)\n        dispatch_sent_time_utc = request_meta.get(\"dispatch_sent_time_utc\")\n        dispatch_sent_dt = _parse_utc_timestamp(dispatch_sent_time_utc)\n        event_id = request_meta.get(\"event_id\")\n\n        cold_start = not _FIRST_INVOKE.get(function_id, False)\n        _FIRST_INVOKE[function_id] = True\n\n        req_bytes = _estimate_request_bytes(request)\n\n        wall_t0 = time.perf_counter()\n        cpu_t0 = time.process_time()\n\n        error: Optional[str] = None\n        error_type: Optional[str] = None\n        rv: Any = None\n\n        try:\n            rv = handler(*args, **kwargs)\n        except Exception as exc:\n            error_type = type(exc).__name__\n            error = str(exc)\n            raise\n        finally:\n            wall_t1 = time.perf_counter()\n            cpu_t1 = time.process_time()\n            response_finished_dt = datetime.now(timezone.utc)\n\n            body_bytes: bytes = b\"\"\n            status_code: Optional[int] = None\n            resp_bytes: Optional[int] = None\n            resp_meta: Optional[dict] = None\n\n            try:\n                body_bytes, status_code, _headers = _normalize_handler_return(rv)\n                resp_bytes = len(body_bytes)\n\n                # If body looks like JSON, parse and keep a sanitized subset for logs\n                try:\n                    parsed = json.loads(body_bytes.decode(\"utf-8\", errors=\"replace\"))\n                    resp_meta = _sanitize_response_json_for_logs(parsed)\n                except Exception:\n                    resp_meta = None\n            except Exception:\n                # Never let metrics break the request path.\n                pass\n\n            queue_delay_ms = None\n            end_to_end_latency_ms = None\n            if dispatch_sent_dt:\n                queue_delay = (request_received_dt - dispatch_sent_dt).total_seconds() * 1000.0\n                if queue_delay >= 0:\n                    queue_delay_ms = round(queue_delay, 3)\n                end_to_end_latency = (response_finished_dt - dispatch_sent_dt).total_seconds() * 1000.0\n                if end_to_end_latency >= 0:\n                    end_to_end_latency_ms = round(end_to_end_latency, 3)\n\n            log_line = {\n                \"type\": \"invocation_metrics\",\n                \"function_id\": function_id,\n                \"cold_start\": cold_start,\n                \"event_id\": event_id,\n                \"dispatch_sent_time_utc\": dispatch_sent_time_utc,\n                \"request_received_time_utc\": request_received_dt.isoformat().replace(\"+00:00\", \"Z\"),\n                \"response_finished_time_utc\": response_finished_dt.isoformat().replace(\"+00:00\", \"Z\"),\n                \"queue_delay_ms\": queue_delay_ms,\n                \"end_to_end_latency_ms\": end_to_end_latency_ms,\n                \"process_uptime_s\": round(time.time() - _PROCESS_START_UNIX, 3),\n                \"wall_ms\": round((wall_t1 - wall_t0) * 1000.0, 3),\n                \"cpu_ms\": round((cpu_t1 - cpu_t0) * 1000.0, 3),\n                \"max_rss_kb\": _safe_get_max_rss_kb(),\n                \"request_bytes\": req_bytes,\n                \"response_bytes\": resp_bytes,\n                \"status_code\": status_code,\n                \"error_type\": error_type,\n                \"error\": error,\n                # Helpful Cloud Run env metadata (if present)\n                \"k_service\": os.getenv(\"K_SERVICE\"),\n                \"k_revision\": os.getenv(\"K_REVISION\"),\n                \"k_configuration\": os.getenv(\"K_CONFIGURATION\"),\n                # Optional: request metadata (best-effort)\n                \"http_method\": getattr(request, \"method\", None),\n                \"http_path\": getattr(request, \"path\", None),\n            }\n            if resp_meta is not None:\n                log_line[\"response_meta\"] = resp_meta\n\n            _emit_metrics_log(log_line)\n\n        return rv\n\n    _wrapped.__name__ = getattr(handler, \"__name__\", function_id)\n    _wrapped.__doc__ = getattr(handler, \"__doc__\", None)\n    return _wrapped\n\n# --- Handler module code ---\n\"\"\"Minimal health-check endpoint mimicking high-volume lightweight traffic.\"\"\"\n\nimport json\nfrom datetime import datetime, timezone\nfrom typing import Dict\n\nimport functions_framework\n\n\n@functions_framework.http\n\ndef api_health_check(request) -> tuple[str, int, Dict[str, str]]:\n    \"\"\"Return a compact health snapshot that shows the invocation scenario.\"\"\"\n\n    payload = request.get_json(silent=True) or {}\n    now = datetime.now(timezone.utc).isoformat()\n    result = {\n        \"status\": \"ok\",\n        \"timestamp\": now,\n        \"scenario\": \"Short runtime + Little data\",\n        \"payload\": payload,\n    }\n    return json.dumps(result), 200, {\"Content-Type\": \"application/json\"}\n\n# --- Wrapped entrypoints ---\n_raw_handler = api_health_check\napi_health_check = _with_metrics(\"api_health_check\", _raw_handler)\n\ndef main(request):\n    return api_health_check(request)\n"
    },
    "image_format_converter": {
      "function_id": "image_format_converter",
      "runtime_ms": 5600,
      "memory_mb": 2048,
      "requirements": "Pillow>=10.0.0\ngoogle-cloud-storage>=2.0.0",
      "description": "Transforms uploaded images from PNG (or any byte stream) into WebP/alternative formats, creating tens of megabytes of transfer.",
      "data_input_gb": 0.034,
      "data_output_gb": 0.005,
      "source_location": "us-east1",
      "invocations_per_day": 10,
      "priority": "balanced",
      "latency_important": false,
      "allowed_regions": [
        "europe-north2",
        "us-east1",
        "northamerica-northeast1",
        "us-central1",
        "europe-west1"
      ],
      "code": "#!/usr/bin/env python3\n\"\"\"Autogenerated MCP bundle for image_format_converter (includes metrics wrapper).\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\n# --- Metrics wrapper (from main.py) ---\nFunctionCallable = Callable[..., Any]\n\n_PROCESS_START_UNIX = time.time()\n\n_FIRST_INVOKE: dict[str, bool] = {}\n\ndef _safe_get_max_rss_kb() -> Optional[int]:\n    \"\"\"Best-effort max RSS in KB (Linux: ru_maxrss is KB).\"\"\"\n    try:\n        import resource  # stdlib on Linux\n\n        return int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n    except Exception:\n        return None\n\ndef _estimate_request_bytes(request: Any) -> Optional[int]:\n    \"\"\"Estimate incoming request bytes without breaking downstream handlers.\"\"\"\n    try:\n        cl = getattr(request, \"content_length\", None)\n        if isinstance(cl, int) and cl >= 0:\n            return cl\n    except Exception:\n        pass\n\n    # Fall back to reading request body with caching enabled, so handlers can still read it.\n    try:\n        if hasattr(request, \"get_data\"):\n            data = request.get_data(cache=True)  # important: cache=True\n            if data is not None:\n                return int(len(data))\n    except Exception:\n        pass\n\n    return None\n\ndef _normalize_handler_return(rv: Any) -> Tuple[bytes, int, Dict[str, str]]:\n    \"\"\"\n    Normalize common Functions Framework return shapes:\n      - (body, status, headers)\n      - (body, status)\n      - body\n      - flask.Response-like object\n    \"\"\"\n    body: Any = rv\n    status: int = 200\n    headers: Dict[str, str] = {}\n\n    # Tuple/list return\n    if isinstance(rv, (tuple, list)):\n        if len(rv) >= 1:\n            body = rv[0]\n        if len(rv) >= 2 and rv[1] is not None:\n            try:\n                status = int(rv[1])\n            except Exception:\n                status = 200\n        if len(rv) >= 3 and isinstance(rv[2], dict):\n            headers = {str(k): str(v) for k, v in rv[2].items()}\n\n    # flask.Response-like\n    if hasattr(body, \"get_data\") and callable(getattr(body, \"get_data\")):\n        try:\n            data = body.get_data()  # bytes\n            st = getattr(body, \"status_code\", None)\n            if isinstance(st, int):\n                status = st\n            hdrs = getattr(body, \"headers\", None)\n            if hdrs is not None:\n                try:\n                    headers = {str(k): str(v) for k, v in dict(hdrs).items()}\n                except Exception:\n                    pass\n            return bytes(data), status, headers\n        except Exception:\n            pass\n\n    # Body encoding\n    if body is None:\n        body_bytes = b\"\"\n    elif isinstance(body, (bytes, bytearray)):\n        body_bytes = bytes(body)\n    elif isinstance(body, str):\n        body_bytes = body.encode(\"utf-8\", errors=\"replace\")\n    elif isinstance(body, (dict, list)):\n        body_bytes = json.dumps(body, ensure_ascii=False).encode(\"utf-8\")\n    else:\n        body_bytes = str(body).encode(\"utf-8\", errors=\"replace\")\n\n    return body_bytes, status, headers\n\ndef _sanitize_response_json_for_logs(parsed: Any) -> Optional[dict]:\n    \"\"\"\n    If response is JSON, keep small metadata fields but avoid logging huge blobs (base64, etc.).\n    \"\"\"\n    if not isinstance(parsed, dict):\n        return None\n\n    drop_keys = {\n        \"converted_image\",\n        \"processed_data\",\n        \"converted_image_base64\",\n        \"processed_data_base64\",\n    }  # large base64 fields in your samples\n    out: dict[str, Any] = {}\n\n    for k, v in parsed.items():\n        if k in drop_keys:\n            continue\n        # Avoid very large strings in logs\n        if isinstance(v, str) and len(v) > 512:\n            out[k] = v[:512] + \"...(truncated)\"\n        else:\n            out[k] = v\n    return out\n\ndef _emit_metrics_log(line: dict) -> None:\n    \"\"\"\n    Emit one structured JSON log line to stdout.\n    Cloud Run/Cloud Logging will ingest it as a structured payload.\n    \"\"\"\n    try:\n        print(json.dumps(line, ensure_ascii=False))\n    except Exception:\n        # Best-effort fallback\n        print(str(line))\n\ndef _parse_utc_timestamp(value: Any) -> Optional[datetime]:\n    if value is None:\n        return None\n    if isinstance(value, datetime):\n        dt = value\n    elif isinstance(value, str):\n        text = value.replace(\"Z\", \"+00:00\")\n        try:\n            dt = datetime.fromisoformat(text)\n        except ValueError:\n            return None\n    else:\n        return None\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=timezone.utc)\n    return dt.astimezone(timezone.utc)\n\ndef _extract_request_metadata(request: Any) -> Dict[str, Optional[str]]:\n    event_id = None\n    dispatch_sent_time_utc = None\n\n    if request is None:\n        return {\"event_id\": None, \"dispatch_sent_time_utc\": None}\n\n    payload = None\n    try:\n        if hasattr(request, \"get_json\"):\n            payload = request.get_json(silent=True)\n    except Exception:\n        payload = None\n\n    if isinstance(payload, dict):\n        event_id = payload.get(\"event_id\")\n        dispatch_sent_time_utc = payload.get(\"dispatch_sent_time_utc\")\n\n    try:\n        args = getattr(request, \"args\", None)\n        if args is not None:\n            if event_id is None:\n                event_id = args.get(\"event_id\")\n            if dispatch_sent_time_utc is None:\n                dispatch_sent_time_utc = args.get(\"dispatch_sent_time_utc\")\n    except Exception:\n        pass\n\n    return {\n        \"event_id\": event_id,\n        \"dispatch_sent_time_utc\": dispatch_sent_time_utc,\n    }\n\ndef _with_metrics(function_id: str, handler: FunctionCallable) -> FunctionCallable:\n    \"\"\"Wrap a Functions Framework handler with timing/size/memory instrumentation.\"\"\"\n\n    def _wrapped(*args: Any, **kwargs: Any) -> Any:\n        request = args[0] if args else kwargs.get(\"request\")\n        request_received_dt = datetime.now(timezone.utc)\n        request_meta = _extract_request_metadata(request)\n        dispatch_sent_time_utc = request_meta.get(\"dispatch_sent_time_utc\")\n        dispatch_sent_dt = _parse_utc_timestamp(dispatch_sent_time_utc)\n        event_id = request_meta.get(\"event_id\")\n\n        cold_start = not _FIRST_INVOKE.get(function_id, False)\n        _FIRST_INVOKE[function_id] = True\n\n        req_bytes = _estimate_request_bytes(request)\n\n        wall_t0 = time.perf_counter()\n        cpu_t0 = time.process_time()\n\n        error: Optional[str] = None\n        error_type: Optional[str] = None\n        rv: Any = None\n\n        try:\n            rv = handler(*args, **kwargs)\n        except Exception as exc:\n            error_type = type(exc).__name__\n            error = str(exc)\n            raise\n        finally:\n            wall_t1 = time.perf_counter()\n            cpu_t1 = time.process_time()\n            response_finished_dt = datetime.now(timezone.utc)\n\n            body_bytes: bytes = b\"\"\n            status_code: Optional[int] = None\n            resp_bytes: Optional[int] = None\n            resp_meta: Optional[dict] = None\n\n            try:\n                body_bytes, status_code, _headers = _normalize_handler_return(rv)\n                resp_bytes = len(body_bytes)\n\n                # If body looks like JSON, parse and keep a sanitized subset for logs\n                try:\n                    parsed = json.loads(body_bytes.decode(\"utf-8\", errors=\"replace\"))\n                    resp_meta = _sanitize_response_json_for_logs(parsed)\n                except Exception:\n                    resp_meta = None\n            except Exception:\n                # Never let metrics break the request path.\n                pass\n\n            queue_delay_ms = None\n            end_to_end_latency_ms = None\n            if dispatch_sent_dt:\n                queue_delay = (request_received_dt - dispatch_sent_dt).total_seconds() * 1000.0\n                if queue_delay >= 0:\n                    queue_delay_ms = round(queue_delay, 3)\n                end_to_end_latency = (response_finished_dt - dispatch_sent_dt).total_seconds() * 1000.0\n                if end_to_end_latency >= 0:\n                    end_to_end_latency_ms = round(end_to_end_latency, 3)\n\n            log_line = {\n                \"type\": \"invocation_metrics\",\n                \"function_id\": function_id,\n                \"cold_start\": cold_start,\n                \"event_id\": event_id,\n                \"dispatch_sent_time_utc\": dispatch_sent_time_utc,\n                \"request_received_time_utc\": request_received_dt.isoformat().replace(\"+00:00\", \"Z\"),\n                \"response_finished_time_utc\": response_finished_dt.isoformat().replace(\"+00:00\", \"Z\"),\n                \"queue_delay_ms\": queue_delay_ms,\n                \"end_to_end_latency_ms\": end_to_end_latency_ms,\n                \"process_uptime_s\": round(time.time() - _PROCESS_START_UNIX, 3),\n                \"wall_ms\": round((wall_t1 - wall_t0) * 1000.0, 3),\n                \"cpu_ms\": round((cpu_t1 - cpu_t0) * 1000.0, 3),\n                \"max_rss_kb\": _safe_get_max_rss_kb(),\n                \"request_bytes\": req_bytes,\n                \"response_bytes\": resp_bytes,\n                \"status_code\": status_code,\n                \"error_type\": error_type,\n                \"error\": error,\n                # Helpful Cloud Run env metadata (if present)\n                \"k_service\": os.getenv(\"K_SERVICE\"),\n                \"k_revision\": os.getenv(\"K_REVISION\"),\n                \"k_configuration\": os.getenv(\"K_CONFIGURATION\"),\n                # Optional: request metadata (best-effort)\n                \"http_method\": getattr(request, \"method\", None),\n                \"http_path\": getattr(request, \"path\", None),\n            }\n            if resp_meta is not None:\n                log_line[\"response_meta\"] = resp_meta\n\n            _emit_metrics_log(log_line)\n\n        return rv\n\n    _wrapped.__name__ = getattr(handler, \"__name__\", function_id)\n    _wrapped.__doc__ = getattr(handler, \"__doc__\", None)\n    return _wrapped\n\n# --- Handler module code ---\n\"\"\"Image converter for evaluation runs.\n\nEvaluation improvements:\n- Default behavior writes converted output to GCS and returns only metadata + output URI.\n- Inline (base64) output is still available via `return_inline: true` for debugging.\n\nInputs supported (JSON only):\n- base64 field `data`\n- pointer: `gcs_uri=gs://bucket/path` or (`bucket`, `object`)\n\nRequest JSON (selected fields):\n- format / output_format: e.g. WEBP, PNG, JPEG (default: WEBP)\n- quality: 30..100 (default: 80)\n- return_inline: bool (default: false)\n- output_gcs_uri: gs://bucket/path (optional)\n- output_bucket / output_object: alternative output location (optional)\n- output_prefix: if output not specified and input came from GCS or DEFAULT_OUTPUT_BUCKET is set,\n  write to {bucket}/{output_prefix}/<uuid>.<ext> (default prefix: eval_outputs/image_format_converter)\n\nEnvironment variables (optional):\n- DEFAULT_OUTPUT_BUCKET: bucket used if output location is not provided and input is not from GCS\n- DEFAULT_OUTPUT_PREFIX_IMAGE: default output prefix (default: eval_outputs/image_format_converter)\n- MAX_INLINE_MB: inline response limit in MB (default: 16)\n\"\"\"\n\n\nimport base64\nimport binascii\nimport io\nimport json\nimport os\nimport uuid\nfrom typing import Dict, Optional, Tuple\n\nimport functions_framework\nfrom google.cloud import storage\nfrom PIL import Image\n\n_STORAGE_CLIENT: Optional[storage.Client] = None\n_MAX_INLINE_BYTES: int = 16 * 1024 * 1024\n\ntry:\n    _MAX_INLINE_BYTES = max(1, int(os.getenv(\"MAX_INLINE_MB\", \"16\"))) * 1024 * 1024\nexcept Exception:\n    _MAX_INLINE_BYTES = 16 * 1024 * 1024\n\n_DEFAULT_OUTPUT_BUCKET = os.getenv(\"DEFAULT_OUTPUT_BUCKET\", \"\").strip() or None\n_DEFAULT_OUTPUT_PREFIX = os.getenv(\n    \"DEFAULT_OUTPUT_PREFIX_IMAGE\", \"eval_outputs/image_format_converter\"\n).strip()\n\n\ndef _parse_gcs_uri(uri: Optional[str]) -> Optional[Tuple[str, str]]:\n    if not uri:\n        return None\n    if uri.startswith(\"gs://\"):\n        uri = uri[5:]\n    parts = uri.split(\"/\", 1)\n    if len(parts) == 2 and parts[0] and parts[1]:\n        return parts[0], parts[1]\n    return None\n\n\ndef _get_storage_client() -> storage.Client:\n    global _STORAGE_CLIENT\n    if _STORAGE_CLIENT is None:\n        _STORAGE_CLIENT = storage.Client()\n    return _STORAGE_CLIENT\n\n\ndef _download_from_bucket(bucket_name: str, object_path: str) -> bytes:\n    bucket = _get_storage_client().bucket(bucket_name)\n    blob = bucket.blob(object_path)\n    return blob.download_as_bytes()\n\n\ndef _upload_to_bucket(bucket_name: str, object_path: str, data: bytes, content_type: str) -> str:\n    bucket = _get_storage_client().bucket(bucket_name)\n    blob = bucket.blob(object_path)\n    blob.upload_from_string(data, content_type=content_type)\n    return f\"gs://{bucket_name}/{object_path}\"\n\n\ndef _load_image_bytes(payload: dict) -> Tuple[bytes, Optional[str]]:\n    \"\"\"Return (bytes, input_gcs_uri_if_any).\"\"\"\n    encoded = payload.get(\"data\")\n    if isinstance(encoded, str):\n        try:\n            return base64.b64decode(encoded, validate=True), None\n        except (ValueError, binascii.Error):\n            raise ValueError(\"Invalid base64 in `data`.\")\n\n    gcs_location = _parse_gcs_uri(payload.get(\"gcs_uri\"))\n    bucket = payload.get(\"bucket\")\n    object_path = payload.get(\"object\")\n    if gcs_location:\n        bucket, object_path = gcs_location\n    if bucket and object_path:\n        return _download_from_bucket(str(bucket), str(object_path)), f\"gs://{bucket}/{object_path}\"\n\n    raise ValueError(\n        \"Send JSON with base64 `data`, or specify `bucket`/`object` or `gcs_uri=gs://bucket/path`.\"\n    )\n\n\ndef _pick_output_location(payload: dict, input_gcs_uri: Optional[str], ext: str) -> Tuple[str, str]:\n    # Explicit output_gcs_uri\n    out_gcs = payload.get(\"output_gcs_uri\") or payload.get(\"output_gcs\")\n    parsed = _parse_gcs_uri(out_gcs) if isinstance(out_gcs, str) else None\n    if parsed:\n        return parsed\n\n    # Explicit output_bucket/object\n    out_bucket = payload.get(\"output_bucket\")\n    out_object = payload.get(\"output_object\")\n    if out_bucket and out_object:\n        return str(out_bucket), str(out_object)\n\n    # Derive from input bucket if possible\n    derived_bucket: Optional[str] = None\n    if input_gcs_uri:\n        parsed_in = _parse_gcs_uri(input_gcs_uri)\n        if parsed_in:\n            derived_bucket = parsed_in[0]\n    if not derived_bucket:\n        derived_bucket = _DEFAULT_OUTPUT_BUCKET\n\n    if not derived_bucket:\n        raise ValueError(\n            \"No output location specified. Provide `output_gcs_uri` or (`output_bucket`,`output_object`), \"\n            \"or set DEFAULT_OUTPUT_BUCKET env var, or provide input via GCS so output can be derived.\"\n        )\n\n    prefix = str(payload.get(\"output_prefix\") or _DEFAULT_OUTPUT_PREFIX).strip().strip(\"/\")\n    object_path = f\"{prefix}/{uuid.uuid4().hex}.{ext}\"\n    return derived_bucket, object_path\n\n\n@functions_framework.http\ndef image_format_converter(request) -> tuple[str, int, Dict[str, str]]:\n    \"\"\"Convert uploaded image bytes into the requested format.\"\"\"\n    payload = request.get_json(silent=True)\n    if not isinstance(payload, dict):\n        return json.dumps({\"error\": \"Expected a JSON object payload.\"}), 400, {\"Content-Type\": \"application/json\"}\n\n    # Accept both keys for compatibility.\n    target_format = str(payload.get(\"format\", payload.get(\"output_format\", \"WEBP\"))).upper()\n    quality = int(payload.get(\"quality\", 80))\n    quality = max(30, min(quality, 100))\n\n    return_inline = bool(payload.get(\"return_inline\", False))\n\n    try:\n        input_bytes, input_gcs_uri = _load_image_bytes(payload)\n    except ValueError as error:\n        return json.dumps({\"error\": str(error)}), 400, {\"Content-Type\": \"application/json\"}\n\n    try:\n        image = Image.open(io.BytesIO(input_bytes))\n    except Exception as error:\n        return json.dumps({\"error\": f\"Failed to decode image: {error}\"}), 400, {\"Content-Type\": \"application/json\"}\n\n    output_buffer = io.BytesIO()\n    try:\n        image.save(output_buffer, format=target_format, quality=quality)\n    except Exception as error:\n        return (\n            json.dumps({\"error\": f\"Failed to encode image as {target_format}: {error}\"}),\n            400,\n            {\"Content-Type\": \"application/json\"},\n        )\n\n    output_bytes = output_buffer.getvalue()\n\n    response: dict = {\n        \"scenario\": \"Short runtime + Large data\",\n        \"format\": target_format,\n        \"quality\": quality,\n        \"input_bytes\": len(input_bytes),\n        \"output_bytes\": len(output_bytes),\n        \"input_size_mb\": round(len(input_bytes) / (1024 * 1024), 4),\n        \"output_size_mb\": round(len(output_bytes) / (1024 * 1024), 4),\n        \"input_gcs_uri\": input_gcs_uri,\n        \"return_inline\": return_inline,\n    }\n\n    if return_inline:\n        if len(output_bytes) > _MAX_INLINE_BYTES:\n            return (\n                json.dumps(\n                    {\n                        \"error\": \"Inline response too large.\",\n                        \"hint\": \"Use GCS output or lower the input size.\",\n                        \"max_inline_bytes\": _MAX_INLINE_BYTES,\n                    }\n                ),\n                413,\n                {\"Content-Type\": \"application/json\"},\n            )\n        # Debug mode only (can be large).\n        response[\"converted_image_base64\"] = base64.b64encode(output_bytes).decode(\"utf-8\")\n        return json.dumps(response), 200, {\"Content-Type\": \"application/json\"}\n\n    # Evaluation mode: write output to GCS and return URI + metadata.\n    ext = target_format.lower()\n    if ext == \"jpeg\":\n        ext = \"jpg\"\n\n    try:\n        out_bucket, out_object = _pick_output_location(payload, input_gcs_uri, ext)\n        content_type = f\"image/{'jpeg' if ext == 'jpg' else ext}\"\n        output_gcs_uri = _upload_to_bucket(out_bucket, out_object, output_bytes, content_type=content_type)\n        response[\"output_gcs_uri\"] = output_gcs_uri\n        response[\"output_bucket\"] = out_bucket\n        response[\"output_object\"] = out_object\n    except ValueError as error:\n        return (\n            json.dumps(\n                {\n                    \"error\": str(error),\n                    \"hint\": \"Set output_gcs_uri or output_bucket/output_object or DEFAULT_OUTPUT_BUCKET.\",\n                }\n            ),\n            400,\n            {\"Content-Type\": \"application/json\"},\n        )\n    except Exception as error:\n        return json.dumps({\"error\": f\"Failed to upload output to GCS: {error}\"}), 500, {\"Content-Type\": \"application/json\"}\n\n    return json.dumps(response), 200, {\"Content-Type\": \"application/json\"}\n\n# --- Wrapped entrypoints ---\n_raw_handler = image_format_converter\nimage_format_converter = _with_metrics(\"image_format_converter\", _raw_handler)\n\ndef main(request):\n    return image_format_converter(request)\n"
    },
    "crypto_key_gen": {
      "function_id": "crypto_key_gen",
      "runtime_ms": 10000,
      "memory_mb": 4096,
      "requirements": "cryptography>=41.0.0",
      "description": "Generate RSA key pairs to exercise sustained CPU work while keeping the input/output payloads minuscule.",
      "data_input_gb": 1e-05,
      "data_output_gb": 1e-05,
      "gpu_required": true,
      "source_location": "us-east1",
      "invocations_per_day": 30,
      "priority": "balanced",
      "latency_important": false,
      "allowed_regions": [
        "europe-north2",
        "us-east1",
        "northamerica-northeast1",
        "us-central1",
        "europe-west1"
      ],
      "code": "#!/usr/bin/env python3\n\"\"\"Autogenerated MCP bundle for crypto_key_gen (includes metrics wrapper).\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\n# --- Metrics wrapper (from main.py) ---\nFunctionCallable = Callable[..., Any]\n\n_PROCESS_START_UNIX = time.time()\n\n_FIRST_INVOKE: dict[str, bool] = {}\n\ndef _safe_get_max_rss_kb() -> Optional[int]:\n    \"\"\"Best-effort max RSS in KB (Linux: ru_maxrss is KB).\"\"\"\n    try:\n        import resource  # stdlib on Linux\n\n        return int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n    except Exception:\n        return None\n\ndef _estimate_request_bytes(request: Any) -> Optional[int]:\n    \"\"\"Estimate incoming request bytes without breaking downstream handlers.\"\"\"\n    try:\n        cl = getattr(request, \"content_length\", None)\n        if isinstance(cl, int) and cl >= 0:\n            return cl\n    except Exception:\n        pass\n\n    # Fall back to reading request body with caching enabled, so handlers can still read it.\n    try:\n        if hasattr(request, \"get_data\"):\n            data = request.get_data(cache=True)  # important: cache=True\n            if data is not None:\n                return int(len(data))\n    except Exception:\n        pass\n\n    return None\n\ndef _normalize_handler_return(rv: Any) -> Tuple[bytes, int, Dict[str, str]]:\n    \"\"\"\n    Normalize common Functions Framework return shapes:\n      - (body, status, headers)\n      - (body, status)\n      - body\n      - flask.Response-like object\n    \"\"\"\n    body: Any = rv\n    status: int = 200\n    headers: Dict[str, str] = {}\n\n    # Tuple/list return\n    if isinstance(rv, (tuple, list)):\n        if len(rv) >= 1:\n            body = rv[0]\n        if len(rv) >= 2 and rv[1] is not None:\n            try:\n                status = int(rv[1])\n            except Exception:\n                status = 200\n        if len(rv) >= 3 and isinstance(rv[2], dict):\n            headers = {str(k): str(v) for k, v in rv[2].items()}\n\n    # flask.Response-like\n    if hasattr(body, \"get_data\") and callable(getattr(body, \"get_data\")):\n        try:\n            data = body.get_data()  # bytes\n            st = getattr(body, \"status_code\", None)\n            if isinstance(st, int):\n                status = st\n            hdrs = getattr(body, \"headers\", None)\n            if hdrs is not None:\n                try:\n                    headers = {str(k): str(v) for k, v in dict(hdrs).items()}\n                except Exception:\n                    pass\n            return bytes(data), status, headers\n        except Exception:\n            pass\n\n    # Body encoding\n    if body is None:\n        body_bytes = b\"\"\n    elif isinstance(body, (bytes, bytearray)):\n        body_bytes = bytes(body)\n    elif isinstance(body, str):\n        body_bytes = body.encode(\"utf-8\", errors=\"replace\")\n    elif isinstance(body, (dict, list)):\n        body_bytes = json.dumps(body, ensure_ascii=False).encode(\"utf-8\")\n    else:\n        body_bytes = str(body).encode(\"utf-8\", errors=\"replace\")\n\n    return body_bytes, status, headers\n\ndef _sanitize_response_json_for_logs(parsed: Any) -> Optional[dict]:\n    \"\"\"\n    If response is JSON, keep small metadata fields but avoid logging huge blobs (base64, etc.).\n    \"\"\"\n    if not isinstance(parsed, dict):\n        return None\n\n    drop_keys = {\n        \"converted_image\",\n        \"processed_data\",\n        \"converted_image_base64\",\n        \"processed_data_base64\",\n    }  # large base64 fields in your samples\n    out: dict[str, Any] = {}\n\n    for k, v in parsed.items():\n        if k in drop_keys:\n            continue\n        # Avoid very large strings in logs\n        if isinstance(v, str) and len(v) > 512:\n            out[k] = v[:512] + \"...(truncated)\"\n        else:\n            out[k] = v\n    return out\n\ndef _emit_metrics_log(line: dict) -> None:\n    \"\"\"\n    Emit one structured JSON log line to stdout.\n    Cloud Run/Cloud Logging will ingest it as a structured payload.\n    \"\"\"\n    try:\n        print(json.dumps(line, ensure_ascii=False))\n    except Exception:\n        # Best-effort fallback\n        print(str(line))\n\ndef _parse_utc_timestamp(value: Any) -> Optional[datetime]:\n    if value is None:\n        return None\n    if isinstance(value, datetime):\n        dt = value\n    elif isinstance(value, str):\n        text = value.replace(\"Z\", \"+00:00\")\n        try:\n            dt = datetime.fromisoformat(text)\n        except ValueError:\n            return None\n    else:\n        return None\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=timezone.utc)\n    return dt.astimezone(timezone.utc)\n\ndef _extract_request_metadata(request: Any) -> Dict[str, Optional[str]]:\n    event_id = None\n    dispatch_sent_time_utc = None\n\n    if request is None:\n        return {\"event_id\": None, \"dispatch_sent_time_utc\": None}\n\n    payload = None\n    try:\n        if hasattr(request, \"get_json\"):\n            payload = request.get_json(silent=True)\n    except Exception:\n        payload = None\n\n    if isinstance(payload, dict):\n        event_id = payload.get(\"event_id\")\n        dispatch_sent_time_utc = payload.get(\"dispatch_sent_time_utc\")\n\n    try:\n        args = getattr(request, \"args\", None)\n        if args is not None:\n            if event_id is None:\n                event_id = args.get(\"event_id\")\n            if dispatch_sent_time_utc is None:\n                dispatch_sent_time_utc = args.get(\"dispatch_sent_time_utc\")\n    except Exception:\n        pass\n\n    return {\n        \"event_id\": event_id,\n        \"dispatch_sent_time_utc\": dispatch_sent_time_utc,\n    }\n\ndef _with_metrics(function_id: str, handler: FunctionCallable) -> FunctionCallable:\n    \"\"\"Wrap a Functions Framework handler with timing/size/memory instrumentation.\"\"\"\n\n    def _wrapped(*args: Any, **kwargs: Any) -> Any:\n        request = args[0] if args else kwargs.get(\"request\")\n        request_received_dt = datetime.now(timezone.utc)\n        request_meta = _extract_request_metadata(request)\n        dispatch_sent_time_utc = request_meta.get(\"dispatch_sent_time_utc\")\n        dispatch_sent_dt = _parse_utc_timestamp(dispatch_sent_time_utc)\n        event_id = request_meta.get(\"event_id\")\n\n        cold_start = not _FIRST_INVOKE.get(function_id, False)\n        _FIRST_INVOKE[function_id] = True\n\n        req_bytes = _estimate_request_bytes(request)\n\n        wall_t0 = time.perf_counter()\n        cpu_t0 = time.process_time()\n\n        error: Optional[str] = None\n        error_type: Optional[str] = None\n        rv: Any = None\n\n        try:\n            rv = handler(*args, **kwargs)\n        except Exception as exc:\n            error_type = type(exc).__name__\n            error = str(exc)\n            raise\n        finally:\n            wall_t1 = time.perf_counter()\n            cpu_t1 = time.process_time()\n            response_finished_dt = datetime.now(timezone.utc)\n\n            body_bytes: bytes = b\"\"\n            status_code: Optional[int] = None\n            resp_bytes: Optional[int] = None\n            resp_meta: Optional[dict] = None\n\n            try:\n                body_bytes, status_code, _headers = _normalize_handler_return(rv)\n                resp_bytes = len(body_bytes)\n\n                # If body looks like JSON, parse and keep a sanitized subset for logs\n                try:\n                    parsed = json.loads(body_bytes.decode(\"utf-8\", errors=\"replace\"))\n                    resp_meta = _sanitize_response_json_for_logs(parsed)\n                except Exception:\n                    resp_meta = None\n            except Exception:\n                # Never let metrics break the request path.\n                pass\n\n            queue_delay_ms = None\n            end_to_end_latency_ms = None\n            if dispatch_sent_dt:\n                queue_delay = (request_received_dt - dispatch_sent_dt).total_seconds() * 1000.0\n                if queue_delay >= 0:\n                    queue_delay_ms = round(queue_delay, 3)\n                end_to_end_latency = (response_finished_dt - dispatch_sent_dt).total_seconds() * 1000.0\n                if end_to_end_latency >= 0:\n                    end_to_end_latency_ms = round(end_to_end_latency, 3)\n\n            log_line = {\n                \"type\": \"invocation_metrics\",\n                \"function_id\": function_id,\n                \"cold_start\": cold_start,\n                \"event_id\": event_id,\n                \"dispatch_sent_time_utc\": dispatch_sent_time_utc,\n                \"request_received_time_utc\": request_received_dt.isoformat().replace(\"+00:00\", \"Z\"),\n                \"response_finished_time_utc\": response_finished_dt.isoformat().replace(\"+00:00\", \"Z\"),\n                \"queue_delay_ms\": queue_delay_ms,\n                \"end_to_end_latency_ms\": end_to_end_latency_ms,\n                \"process_uptime_s\": round(time.time() - _PROCESS_START_UNIX, 3),\n                \"wall_ms\": round((wall_t1 - wall_t0) * 1000.0, 3),\n                \"cpu_ms\": round((cpu_t1 - cpu_t0) * 1000.0, 3),\n                \"max_rss_kb\": _safe_get_max_rss_kb(),\n                \"request_bytes\": req_bytes,\n                \"response_bytes\": resp_bytes,\n                \"status_code\": status_code,\n                \"error_type\": error_type,\n                \"error\": error,\n                # Helpful Cloud Run env metadata (if present)\n                \"k_service\": os.getenv(\"K_SERVICE\"),\n                \"k_revision\": os.getenv(\"K_REVISION\"),\n                \"k_configuration\": os.getenv(\"K_CONFIGURATION\"),\n                # Optional: request metadata (best-effort)\n                \"http_method\": getattr(request, \"method\", None),\n                \"http_path\": getattr(request, \"path\", None),\n            }\n            if resp_meta is not None:\n                log_line[\"response_meta\"] = resp_meta\n\n            _emit_metrics_log(log_line)\n\n        return rv\n\n    _wrapped.__name__ = getattr(handler, \"__name__\", function_id)\n    _wrapped.__doc__ = getattr(handler, \"__doc__\", None)\n    return _wrapped\n\n# --- Handler module code ---\n\"\"\"Generate RSA key pairs to exercise sustained CPU work (tunable).\n\nEvaluation-oriented behavior:\n- Keeps response small (no private key material).\n- Allows tuning runtime via `iterations` and/or `target_ms`.\n\nRequest JSON (all optional):\n- bits / key_bits: one of {2048, 3072, 4096, 8192} (default: 3072)\n- public_exponent: RSA public exponent (default: 65537)\n- iterations: number of RSA keypairs to generate (default: 1)\n- target_ms: minimum wall time to spend (default: 0 => no minimum)\n- burn_chunk_kb: chunk size for CPU-burn hashing (default: 256)\n- return_public_key_pem: whether to return full public key PEM (default: false)\n\nNotes:\n- If `target_ms` exceeds the time needed for RSA generation, we burn CPU time by hashing\n  a rolling buffer until the minimum wall time is reached. This produces more stable\n  runtimes across regions/CPU allocations than relying purely on RSA generation loops.\n\"\"\"\n\n\nimport hashlib\nimport json\nimport os\nimport time\nfrom typing import Dict, Tuple\n\nimport functions_framework\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\n\n_ALLOWED_BITS = (2048, 3072, 4096, 8192)\n\n\ndef _parse_bits(payload: dict) -> int:\n    bits = payload.get(\"bits\", payload.get(\"key_bits\", 3072))\n    try:\n        bits = int(bits)\n    except Exception:\n        bits = 3072\n    if bits not in _ALLOWED_BITS:\n        bits = 3072\n    return bits\n\n\ndef _parse_int(payload: dict, key: str, default: int, minimum: int | None = None, maximum: int | None = None) -> int:\n    try:\n        val = int(payload.get(key, default))\n    except Exception:\n        val = default\n    if minimum is not None:\n        val = max(minimum, val)\n    if maximum is not None:\n        val = min(maximum, val)\n    return val\n\n\ndef _burn_cpu_until(deadline_s: float, chunk_kb: int) -> Tuple[int, str]:\n    \"\"\"Burn CPU by hashing a rolling buffer; returns (hash_iterations, final_digest).\"\"\"\n    data = bytearray(os.urandom(max(1, chunk_kb) * 1024))\n    digest = b\"\"\n    iters = 0\n    while time.perf_counter() < deadline_s:\n        digest = hashlib.sha256(data).digest()\n        # mutate buffer to prevent trivial caching\n        for i in range(min(32, len(data))):\n            data[i] ^= digest[i % len(digest)]\n        iters += 1\n    return iters, digest.hex() if digest else \"\"\n\n\n@functions_framework.http\ndef crypto_key_gen(request) -> tuple[str, int, Dict[str, str]]:\n    \"\"\"Generate RSA public keys and optionally burn CPU until a target duration is met.\"\"\"\n    payload = request.get_json(silent=True) or {}\n\n    bits = _parse_bits(payload)\n    public_exponent = _parse_int(payload, \"public_exponent\", 65537, minimum=3, maximum=2**31 - 1)\n\n    iterations = _parse_int(payload, \"iterations\", 1, minimum=1, maximum=50)\n    target_ms = _parse_int(payload, \"target_ms\", 0, minimum=0, maximum=15 * 60 * 1000)  # cap 15 min\n    burn_chunk_kb = _parse_int(payload, \"burn_chunk_kb\", 256, minimum=1, maximum=4096)\n    return_public_key_pem = bool(payload.get(\"return_public_key_pem\", False))\n\n    wall_start = time.perf_counter()\n    cpu_start = time.process_time()\n\n    public_keys: list[bytes] = []\n\n    # Step 1: deterministic crypto work\n    for _ in range(iterations):\n        private_key = rsa.generate_private_key(public_exponent=public_exponent, key_size=bits)\n        pub = private_key.public_key()\n        public_bytes = pub.public_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PublicFormat.SubjectPublicKeyInfo,\n        )\n        public_keys.append(public_bytes)\n\n    # Step 2: optional CPU burn to reach target_ms\n    burn_iters = 0\n    burn_digest = \"\"\n    if target_ms > 0:\n        deadline_s = wall_start + (target_ms / 1000.0)\n        if time.perf_counter() < deadline_s:\n            burn_iters, burn_digest = _burn_cpu_until(deadline_s, burn_chunk_kb)\n\n    wall_ms = round((time.perf_counter() - wall_start) * 1000.0, 3)\n    cpu_ms = round((time.process_time() - cpu_start) * 1000.0, 3)\n\n    response: dict = {\n        \"scenario\": \"Long runtime + Little data\",\n        \"key_bits\": bits,\n        \"public_exponent\": public_exponent,\n        \"iterations\": iterations,\n        \"target_ms\": target_ms,\n        \"burn_hash_iterations\": burn_iters,\n        \"burn_final_digest\": burn_digest or None,\n        # Optional internal timings (your main wrapper logs the authoritative ones)\n        \"wall_ms_internal\": wall_ms,\n        \"cpu_ms_internal\": cpu_ms,\n    }\n\n    if return_public_key_pem:\n        # Keep response bounded: if multiple keys were generated, return only the last one.\n        response[\"public_key_pem\"] = public_keys[-1].decode(\"utf-8\", errors=\"ignore\") if public_keys else None\n\n    return json.dumps(response), 200, {\"Content-Type\": \"application/json\"}\n\n# --- Wrapped entrypoints ---\n_raw_handler = crypto_key_gen\ncrypto_key_gen = _with_metrics(\"crypto_key_gen\", _raw_handler)\n\ndef main(request):\n    return crypto_key_gen(request)\n"
    },
    "video_transcoder": {
      "function_id": "video_transcoder",
      "runtime_ms": 15000,
      "memory_mb": 8192,
      "vcpus": 4,
      "requirements": "google-cloud-storage>=2.0.0",
      "description": "Consumes large binary payloads and compresses them in multiple passes to mimic heavy transcoding jobs.",
      "data_input_gb": 0.5,
      "data_output_gb": 0.5,
      "source_location": "us-east1",
      "invocations_per_day": 10,
      "priority": "balanced",
      "latency_important": false,
      "allowed_regions": [
        "europe-north2",
        "us-east1",
        "northamerica-northeast1",
        "us-central1",
        "europe-west1"
      ],
      "code": "#!/usr/bin/env python3\n\"\"\"Autogenerated MCP bundle for video_transcoder (includes metrics wrapper).\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\n# --- Metrics wrapper (from main.py) ---\nFunctionCallable = Callable[..., Any]\n\n_PROCESS_START_UNIX = time.time()\n\n_FIRST_INVOKE: dict[str, bool] = {}\n\ndef _safe_get_max_rss_kb() -> Optional[int]:\n    \"\"\"Best-effort max RSS in KB (Linux: ru_maxrss is KB).\"\"\"\n    try:\n        import resource  # stdlib on Linux\n\n        return int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n    except Exception:\n        return None\n\ndef _estimate_request_bytes(request: Any) -> Optional[int]:\n    \"\"\"Estimate incoming request bytes without breaking downstream handlers.\"\"\"\n    try:\n        cl = getattr(request, \"content_length\", None)\n        if isinstance(cl, int) and cl >= 0:\n            return cl\n    except Exception:\n        pass\n\n    # Fall back to reading request body with caching enabled, so handlers can still read it.\n    try:\n        if hasattr(request, \"get_data\"):\n            data = request.get_data(cache=True)  # important: cache=True\n            if data is not None:\n                return int(len(data))\n    except Exception:\n        pass\n\n    return None\n\ndef _normalize_handler_return(rv: Any) -> Tuple[bytes, int, Dict[str, str]]:\n    \"\"\"\n    Normalize common Functions Framework return shapes:\n      - (body, status, headers)\n      - (body, status)\n      - body\n      - flask.Response-like object\n    \"\"\"\n    body: Any = rv\n    status: int = 200\n    headers: Dict[str, str] = {}\n\n    # Tuple/list return\n    if isinstance(rv, (tuple, list)):\n        if len(rv) >= 1:\n            body = rv[0]\n        if len(rv) >= 2 and rv[1] is not None:\n            try:\n                status = int(rv[1])\n            except Exception:\n                status = 200\n        if len(rv) >= 3 and isinstance(rv[2], dict):\n            headers = {str(k): str(v) for k, v in rv[2].items()}\n\n    # flask.Response-like\n    if hasattr(body, \"get_data\") and callable(getattr(body, \"get_data\")):\n        try:\n            data = body.get_data()  # bytes\n            st = getattr(body, \"status_code\", None)\n            if isinstance(st, int):\n                status = st\n            hdrs = getattr(body, \"headers\", None)\n            if hdrs is not None:\n                try:\n                    headers = {str(k): str(v) for k, v in dict(hdrs).items()}\n                except Exception:\n                    pass\n            return bytes(data), status, headers\n        except Exception:\n            pass\n\n    # Body encoding\n    if body is None:\n        body_bytes = b\"\"\n    elif isinstance(body, (bytes, bytearray)):\n        body_bytes = bytes(body)\n    elif isinstance(body, str):\n        body_bytes = body.encode(\"utf-8\", errors=\"replace\")\n    elif isinstance(body, (dict, list)):\n        body_bytes = json.dumps(body, ensure_ascii=False).encode(\"utf-8\")\n    else:\n        body_bytes = str(body).encode(\"utf-8\", errors=\"replace\")\n\n    return body_bytes, status, headers\n\ndef _sanitize_response_json_for_logs(parsed: Any) -> Optional[dict]:\n    \"\"\"\n    If response is JSON, keep small metadata fields but avoid logging huge blobs (base64, etc.).\n    \"\"\"\n    if not isinstance(parsed, dict):\n        return None\n\n    drop_keys = {\n        \"converted_image\",\n        \"processed_data\",\n        \"converted_image_base64\",\n        \"processed_data_base64\",\n    }  # large base64 fields in your samples\n    out: dict[str, Any] = {}\n\n    for k, v in parsed.items():\n        if k in drop_keys:\n            continue\n        # Avoid very large strings in logs\n        if isinstance(v, str) and len(v) > 512:\n            out[k] = v[:512] + \"...(truncated)\"\n        else:\n            out[k] = v\n    return out\n\ndef _emit_metrics_log(line: dict) -> None:\n    \"\"\"\n    Emit one structured JSON log line to stdout.\n    Cloud Run/Cloud Logging will ingest it as a structured payload.\n    \"\"\"\n    try:\n        print(json.dumps(line, ensure_ascii=False))\n    except Exception:\n        # Best-effort fallback\n        print(str(line))\n\ndef _parse_utc_timestamp(value: Any) -> Optional[datetime]:\n    if value is None:\n        return None\n    if isinstance(value, datetime):\n        dt = value\n    elif isinstance(value, str):\n        text = value.replace(\"Z\", \"+00:00\")\n        try:\n            dt = datetime.fromisoformat(text)\n        except ValueError:\n            return None\n    else:\n        return None\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=timezone.utc)\n    return dt.astimezone(timezone.utc)\n\ndef _extract_request_metadata(request: Any) -> Dict[str, Optional[str]]:\n    event_id = None\n    dispatch_sent_time_utc = None\n\n    if request is None:\n        return {\"event_id\": None, \"dispatch_sent_time_utc\": None}\n\n    payload = None\n    try:\n        if hasattr(request, \"get_json\"):\n            payload = request.get_json(silent=True)\n    except Exception:\n        payload = None\n\n    if isinstance(payload, dict):\n        event_id = payload.get(\"event_id\")\n        dispatch_sent_time_utc = payload.get(\"dispatch_sent_time_utc\")\n\n    try:\n        args = getattr(request, \"args\", None)\n        if args is not None:\n            if event_id is None:\n                event_id = args.get(\"event_id\")\n            if dispatch_sent_time_utc is None:\n                dispatch_sent_time_utc = args.get(\"dispatch_sent_time_utc\")\n    except Exception:\n        pass\n\n    return {\n        \"event_id\": event_id,\n        \"dispatch_sent_time_utc\": dispatch_sent_time_utc,\n    }\n\ndef _with_metrics(function_id: str, handler: FunctionCallable) -> FunctionCallable:\n    \"\"\"Wrap a Functions Framework handler with timing/size/memory instrumentation.\"\"\"\n\n    def _wrapped(*args: Any, **kwargs: Any) -> Any:\n        request = args[0] if args else kwargs.get(\"request\")\n        request_received_dt = datetime.now(timezone.utc)\n        request_meta = _extract_request_metadata(request)\n        dispatch_sent_time_utc = request_meta.get(\"dispatch_sent_time_utc\")\n        dispatch_sent_dt = _parse_utc_timestamp(dispatch_sent_time_utc)\n        event_id = request_meta.get(\"event_id\")\n\n        cold_start = not _FIRST_INVOKE.get(function_id, False)\n        _FIRST_INVOKE[function_id] = True\n\n        req_bytes = _estimate_request_bytes(request)\n\n        wall_t0 = time.perf_counter()\n        cpu_t0 = time.process_time()\n\n        error: Optional[str] = None\n        error_type: Optional[str] = None\n        rv: Any = None\n\n        try:\n            rv = handler(*args, **kwargs)\n        except Exception as exc:\n            error_type = type(exc).__name__\n            error = str(exc)\n            raise\n        finally:\n            wall_t1 = time.perf_counter()\n            cpu_t1 = time.process_time()\n            response_finished_dt = datetime.now(timezone.utc)\n\n            body_bytes: bytes = b\"\"\n            status_code: Optional[int] = None\n            resp_bytes: Optional[int] = None\n            resp_meta: Optional[dict] = None\n\n            try:\n                body_bytes, status_code, _headers = _normalize_handler_return(rv)\n                resp_bytes = len(body_bytes)\n\n                # If body looks like JSON, parse and keep a sanitized subset for logs\n                try:\n                    parsed = json.loads(body_bytes.decode(\"utf-8\", errors=\"replace\"))\n                    resp_meta = _sanitize_response_json_for_logs(parsed)\n                except Exception:\n                    resp_meta = None\n            except Exception:\n                # Never let metrics break the request path.\n                pass\n\n            queue_delay_ms = None\n            end_to_end_latency_ms = None\n            if dispatch_sent_dt:\n                queue_delay = (request_received_dt - dispatch_sent_dt).total_seconds() * 1000.0\n                if queue_delay >= 0:\n                    queue_delay_ms = round(queue_delay, 3)\n                end_to_end_latency = (response_finished_dt - dispatch_sent_dt).total_seconds() * 1000.0\n                if end_to_end_latency >= 0:\n                    end_to_end_latency_ms = round(end_to_end_latency, 3)\n\n            log_line = {\n                \"type\": \"invocation_metrics\",\n                \"function_id\": function_id,\n                \"cold_start\": cold_start,\n                \"event_id\": event_id,\n                \"dispatch_sent_time_utc\": dispatch_sent_time_utc,\n                \"request_received_time_utc\": request_received_dt.isoformat().replace(\"+00:00\", \"Z\"),\n                \"response_finished_time_utc\": response_finished_dt.isoformat().replace(\"+00:00\", \"Z\"),\n                \"queue_delay_ms\": queue_delay_ms,\n                \"end_to_end_latency_ms\": end_to_end_latency_ms,\n                \"process_uptime_s\": round(time.time() - _PROCESS_START_UNIX, 3),\n                \"wall_ms\": round((wall_t1 - wall_t0) * 1000.0, 3),\n                \"cpu_ms\": round((cpu_t1 - cpu_t0) * 1000.0, 3),\n                \"max_rss_kb\": _safe_get_max_rss_kb(),\n                \"request_bytes\": req_bytes,\n                \"response_bytes\": resp_bytes,\n                \"status_code\": status_code,\n                \"error_type\": error_type,\n                \"error\": error,\n                # Helpful Cloud Run env metadata (if present)\n                \"k_service\": os.getenv(\"K_SERVICE\"),\n                \"k_revision\": os.getenv(\"K_REVISION\"),\n                \"k_configuration\": os.getenv(\"K_CONFIGURATION\"),\n                # Optional: request metadata (best-effort)\n                \"http_method\": getattr(request, \"method\", None),\n                \"http_path\": getattr(request, \"path\", None),\n            }\n            if resp_meta is not None:\n                log_line[\"response_meta\"] = resp_meta\n\n            _emit_metrics_log(log_line)\n\n        return rv\n\n    _wrapped.__name__ = getattr(handler, \"__name__\", function_id)\n    _wrapped.__doc__ = getattr(handler, \"__doc__\", None)\n    return _wrapped\n\n# --- Handler module code ---\n\"\"\"Transcoder-like workload for evaluation runs.\n\nEvaluation improvements:\n- Default behavior writes processed output to GCS and returns only metadata + output URI.\n- Inline (base64) output is still available via `return_inline: true` for debugging.\n- Runtime is tunable via `passes` and `target_ms` (minimum wall time).\n\nInputs supported (JSON only):\n- base64 field `data`\n- pointer: `gcs_uri=gs://bucket/path` or (`bucket`, `object`)\n\nRequest JSON (selected fields):\n- passes: 1..10 (default: 3)\n- target_ms: minimum wall time (default: 0 => no minimum)\n- return_inline: bool (default: false)\n- output_gcs_uri: gs://bucket/path (optional)\n- output_bucket / output_object: alternative output location (optional)\n- output_prefix: if output not specified and input came from GCS or DEFAULT_OUTPUT_BUCKET is set,\n  write to {bucket}/{output_prefix}/<uuid>.bin (default prefix: eval_outputs/video_transcoder)\n\nEnvironment variables (optional):\n- DEFAULT_OUTPUT_BUCKET: bucket used if output location is not provided and input is not from GCS\n- DEFAULT_OUTPUT_PREFIX_VIDEO: default output prefix (default: eval_outputs/video_transcoder)\n- MAX_INLINE_MB: inline response limit in MB (default: 16)\n\"\"\"\n\n\nimport base64\nimport binascii\nimport hashlib\nimport json\nimport os\nimport time\nimport uuid\nimport zlib\nfrom typing import Dict, Optional, Tuple\n\nimport functions_framework\nfrom google.cloud import storage\n\n_STORAGE_CLIENT: Optional[storage.Client] = None\n_MAX_INLINE_BYTES: int = 16 * 1024 * 1024\n\ntry:\n    _MAX_INLINE_BYTES = max(1, int(os.getenv(\"MAX_INLINE_MB\", \"16\"))) * 1024 * 1024\nexcept Exception:\n    _MAX_INLINE_BYTES = 16 * 1024 * 1024\n\n_DEFAULT_OUTPUT_BUCKET = os.getenv(\"DEFAULT_OUTPUT_BUCKET\", \"\").strip() or None\n_DEFAULT_OUTPUT_PREFIX = os.getenv(\n    \"DEFAULT_OUTPUT_PREFIX_VIDEO\", \"eval_outputs/video_transcoder\"\n).strip()\n\n\ndef _parse_gcs_uri(uri: Optional[str]) -> Optional[Tuple[str, str]]:\n    if not uri:\n        return None\n    if uri.startswith(\"gs://\"):\n        uri = uri[5:]\n    parts = uri.split(\"/\", 1)\n    if len(parts) == 2 and parts[0] and parts[1]:\n        return parts[0], parts[1]\n    return None\n\n\ndef _get_storage_client() -> storage.Client:\n    global _STORAGE_CLIENT\n    if _STORAGE_CLIENT is None:\n        _STORAGE_CLIENT = storage.Client()\n    return _STORAGE_CLIENT\n\n\ndef _download_from_bucket(bucket_name: str, object_path: str) -> bytes:\n    bucket = _get_storage_client().bucket(bucket_name)\n    blob = bucket.blob(object_path)\n    return blob.download_as_bytes()\n\n\ndef _upload_to_bucket(bucket_name: str, object_path: str, data: bytes, content_type: str) -> str:\n    bucket = _get_storage_client().bucket(bucket_name)\n    blob = bucket.blob(object_path)\n    blob.upload_from_string(data, content_type=content_type)\n    return f\"gs://{bucket_name}/{object_path}\"\n\n\ndef _extract_payload(payload: dict) -> Tuple[bytes, Optional[str]]:\n    \"\"\"Return (bytes, input_gcs_uri_if_any).\"\"\"\n    encoded = payload.get(\"data\")\n    if isinstance(encoded, str):\n        try:\n            return base64.b64decode(encoded, validate=True), None\n        except (ValueError, binascii.Error):\n            raise ValueError(\"Invalid base64 in `data`.\")\n\n    gcs_location = _parse_gcs_uri(payload.get(\"gcs_uri\"))\n    bucket = payload.get(\"bucket\")\n    object_path = payload.get(\"object\")\n    if gcs_location:\n        bucket, object_path = gcs_location\n    if bucket and object_path:\n        return _download_from_bucket(str(bucket), str(object_path)), f\"gs://{bucket}/{object_path}\"\n\n    raise ValueError(\n        \"Send JSON with base64 `data`, or specify `bucket`/`object` or `gcs_uri=gs://bucket/path`.\"\n    )\n\n\ndef _pick_output_location(payload: dict, input_gcs_uri: Optional[str]) -> Tuple[str, str]:\n    # Explicit output_gcs_uri\n    out_gcs = payload.get(\"output_gcs_uri\") or payload.get(\"output_gcs\")\n    parsed = _parse_gcs_uri(out_gcs) if isinstance(out_gcs, str) else None\n    if parsed:\n        return parsed\n\n    # Explicit output_bucket/object\n    out_bucket = payload.get(\"output_bucket\")\n    out_object = payload.get(\"output_object\")\n    if out_bucket and out_object:\n        return str(out_bucket), str(out_object)\n\n    # Derive from input bucket if possible\n    derived_bucket: Optional[str] = None\n    if input_gcs_uri:\n        parsed_in = _parse_gcs_uri(input_gcs_uri)\n        if parsed_in:\n            derived_bucket = parsed_in[0]\n    if not derived_bucket:\n        derived_bucket = _DEFAULT_OUTPUT_BUCKET\n\n    if not derived_bucket:\n        raise ValueError(\n            \"No output location specified. Provide `output_gcs_uri` or (`output_bucket`,`output_object`), \"\n            \"or set DEFAULT_OUTPUT_BUCKET env var, or provide input via GCS so output can be derived.\"\n        )\n\n    prefix = str(payload.get(\"output_prefix\") or _DEFAULT_OUTPUT_PREFIX).strip().strip(\"/\")\n    object_path = f\"{prefix}/{uuid.uuid4().hex}.bin\"\n    return derived_bucket, object_path\n\n\ndef _compress_pass(data: bytes) -> bytes:\n    compressor = zlib.compressobj(level=6)\n    return compressor.compress(data) + compressor.flush()\n\n\n@functions_framework.http\ndef video_transcoder(request) -> tuple[str, int, Dict[str, str]]:\n    \"\"\"Transcode whichever payload you upload by compressing it multiple times.\"\"\"\n    payload = request.get_json(silent=True)\n    if not isinstance(payload, dict):\n        return json.dumps({\"error\": \"Expected a JSON object payload.\"}), 400, {\"Content-Type\": \"application/json\"}\n\n    passes = int(payload.get(\"passes\", 3))\n    passes = max(1, min(passes, 10))\n\n    target_ms = int(payload.get(\"target_ms\", 0) or 0)\n    target_ms = max(0, min(target_ms, 60 * 60 * 1000))  # cap at 60 min\n\n    return_inline = bool(payload.get(\"return_inline\", False))\n\n    try:\n        raw_bytes, input_gcs_uri = _extract_payload(payload)\n    except ValueError as error:\n        return json.dumps({\"error\": str(error)}), 400, {\"Content-Type\": \"application/json\"}\n\n    start = time.perf_counter()\n    deadline = start + (target_ms / 1000.0) if target_ms > 0 else None\n\n    # Work loop:\n    # - Always do at least one cycle.\n    # - If target_ms is set, repeat cycles until deadline.\n    # - Each cycle starts from original bytes to keep workload stable.\n    cycles = 0\n    total_passes = 0\n    digests: list[str] = []\n\n    processed = raw_bytes\n    while True:\n        cycles += 1\n        processed = raw_bytes\n\n        for _ in range(passes):\n            processed = _compress_pass(processed)\n            total_passes += 1\n\n        digests.append(hashlib.sha256(processed).hexdigest())\n\n        if deadline is None:\n            break\n        if time.perf_counter() >= deadline:\n            break\n        # Safety valve: prevents pathological loops if target_ms is huge and input is tiny.\n        if cycles >= 200:\n            break\n\n    duration_s = round(time.perf_counter() - start, 6)\n\n    response: dict = {\n        \"scenario\": \"Long runtime + Large data\",\n        \"passes\": passes,\n        \"target_ms\": target_ms,\n        \"cycles\": cycles,\n        \"total_passes\": total_passes,\n        \"duration_seconds\": duration_s,\n        \"input_bytes\": len(raw_bytes),\n        \"output_bytes\": len(processed),\n        \"input_size_mb\": round(len(raw_bytes) / (1024 * 1024), 4),\n        \"output_size_mb\": round(len(processed) / (1024 * 1024), 4),\n        \"digest\": digests[-1] if digests else None,\n        \"input_gcs_uri\": input_gcs_uri,\n        \"return_inline\": return_inline,\n    }\n\n    if return_inline:\n        if len(processed) > _MAX_INLINE_BYTES:\n            return (\n                json.dumps(\n                    {\n                        \"error\": \"Inline response too large.\",\n                        \"hint\": \"Use GCS output or lower the input size.\",\n                        \"max_inline_bytes\": _MAX_INLINE_BYTES,\n                    }\n                ),\n                413,\n                {\"Content-Type\": \"application/json\"},\n            )\n        # Debug mode only (can be large).\n        response[\"processed_data_base64\"] = base64.b64encode(processed).decode(\"utf-8\")\n        return json.dumps(response), 200, {\"Content-Type\": \"application/json\"}\n\n    # Evaluation mode: write output to GCS and return URI + metadata.\n    try:\n        out_bucket, out_object = _pick_output_location(payload, input_gcs_uri)\n        output_gcs_uri = _upload_to_bucket(\n            out_bucket, out_object, processed, content_type=\"application/octet-stream\"\n        )\n        response[\"output_gcs_uri\"] = output_gcs_uri\n        response[\"output_bucket\"] = out_bucket\n        response[\"output_object\"] = out_object\n    except ValueError as error:\n        return (\n            json.dumps(\n                {\n                    \"error\": str(error),\n                    \"hint\": \"Set output_gcs_uri or output_bucket/output_object or DEFAULT_OUTPUT_BUCKET.\",\n                }\n            ),\n            400,\n            {\"Content-Type\": \"application/json\"},\n        )\n    except Exception as error:\n        return json.dumps({\"error\": f\"Failed to upload output to GCS: {error}\"}), 500, {\"Content-Type\": \"application/json\"}\n\n    return json.dumps(response), 200, {\"Content-Type\": \"application/json\"}\n\n# --- Wrapped entrypoints ---\n_raw_handler = video_transcoder\nvideo_transcoder = _with_metrics(\"video_transcoder\", _raw_handler)\n\ndef main(request):\n    return video_transcoder(request)\n"
    }
  }
}